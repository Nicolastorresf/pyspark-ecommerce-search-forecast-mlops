{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analyse search terms on the e-commerce web server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this assignment you will download the search term data set for the e-commerce web server and run analytic queries on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.4.tar.gz (311.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.4-py2.py3-none-any.whl size=311905466 sha256=781d3551dbad64106ff3a7c391c4fdc3208f6b10af0510c290a2a0f37341e6f4\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/4e/66/db/939eb1c49afb8a7fd2c4e393ad34e12b77db67bb4cc974c00e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.4\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# Install spark\n",
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import findspark y otras librerías iniciales\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession y SparkContext iniciados.\n",
      "Spark version: 2.4.3\n",
      "SparkContext version: 2.4.3\n"
     ]
    }
   ],
   "source": [
    "# Start session\n",
    "# Crear SparkContext y SparkSession\n",
    "\n",
    "#sc = SparkContext() # A veces iniciar el contexto explícitamente así puede dar problemas si ya existe uno.\n",
    "                     # SparkSession.builder.getOrCreate() usualmente maneja la creación del contexto.\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ECommerceSearchAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext # Obtener el SparkContext de la SparkSession es más seguro\n",
    "\n",
    "print(\"SparkSession y SparkContext iniciados.\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"SparkContext version:\", sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-07 03:48:34--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 233457 (228K) [text/csv]\n",
      "Saving to: ‘searchterms.csv’\n",
      "\n",
      "searchterms.csv     100%[===================>] 227.99K  --.-KB/s    in 0.005s  \n",
      "\n",
      "2025-05-07 03:48:34 (48.4 MB/s) - ‘searchterms.csv’ saved [233457/233457]\n",
      "\n",
      "-rw-r--r-- 1 jupyterlab resources 233457 Sep 29  2022 searchterms.csv\n"
     ]
    }
   ],
   "source": [
    "# Download The search term dataset\n",
    "# https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv\n",
    "\n",
    "# Usaremos un comando de shell para descargar el archivo a la ubicación donde Spark pueda leerlo.\n",
    "# Primero, vamos a definir el nombre del archivo.\n",
    "file_name = \"searchterms.csv\"\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv\"\n",
    "\n",
    "# Descargar usando wget\n",
    "!wget -O {file_name} {url}\n",
    "\n",
    "# Verificar que el archivo se descargó (opcional, pero bueno para depurar)\n",
    "!ls -l {file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+--------------+\n",
      "|day|month|year|    searchterm|\n",
      "+---+-----+----+--------------+\n",
      "| 12|   11|2021| mobile 6 inch|\n",
      "| 12|   11|2021| mobile latest|\n",
      "| 12|   11|2021|   tablet wifi|\n",
      "| 12|   11|2021|laptop 14 inch|\n",
      "| 12|   11|2021|     mobile 5g|\n",
      "+---+-----+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- searchterm: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the csv into a spark dataframe\n",
    "# Spark puede inferir el esquema y sabe que es un CSV con encabezado.\n",
    "\n",
    "# Definir la ruta al archivo descargado\n",
    "# Si la celda anterior lo descargó en el directorio actual del notebook, solo el nombre es suficiente.\n",
    "file_path = file_name # \"searchterms.csv\"\n",
    "\n",
    "# Leer el CSV en un DataFrame de Spark\n",
    "# Es importante decirle a Spark que el CSV tiene una cabecera y que infiera el esquema.\n",
    "search_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Mostrar una muestra para verificar que se cargó correctamente\n",
    "search_df.show(5)\n",
    "search_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame tiene 10000 filas.\n",
      "El DataFrame tiene 4 columnas.\n",
      "Nombres de las columnas: ['day', 'month', 'year', 'searchterm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the number of rows and columns\n",
    "\n",
    "num_rows = search_df.count()\n",
    "num_cols = len(search_df.columns)\n",
    "\n",
    "print(f\"El DataFrame tiene {num_rows} filas.\")\n",
    "print(f\"El DataFrame tiene {num_cols} columnas.\")\n",
    "print(f\"Nombres de las columnas: {search_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las primeras 5 filas del DataFrame son:\n",
      "+---+-----+----+--------------+\n",
      "|day|month|year|    searchterm|\n",
      "+---+-----+----+--------------+\n",
      "| 12|   11|2021| mobile 6 inch|\n",
      "| 12|   11|2021| mobile latest|\n",
      "| 12|   11|2021|   tablet wifi|\n",
      "| 12|   11|2021|laptop 14 inch|\n",
      "| 12|   11|2021|     mobile 5g|\n",
      "+---+-----+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 rows\n",
    "\n",
    "print(\"Las primeras 5 filas del DataFrame son:\")\n",
    "search_df.show(5) # show() por defecto muestra 20, pero podemos especificar 5.\n",
    "# Alternativamente, para obtener una lista de objetos Row:\n",
    "# top_5_rows_list = search_df.head(5)\n",
    "# for row in top_5_rows_list:\n",
    "# print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- searchterm: string (nullable = true)\n",
      "\n",
      "Nombres de columna reales: ['day', 'month', 'year', 'searchterm']\n",
      "El tipo de dato de la columna 'searchterm' es: StringType\n"
     ]
    }
   ],
   "source": [
    "# Find out the datatype of the column searchterm?\n",
    "\n",
    "# Imprimir el esquema completo para ver todos los tipos de datos\n",
    "search_df.printSchema()\n",
    "\n",
    "# Para obtener específicamente el tipo de dato de la columna 'searchterm'\n",
    "# El archivo CSV parece tener columnas como 'day', 'month', 'year', 'searchterm'\n",
    "# Vamos a asumir que la columna de interés se llama 'searchterm'\n",
    "# Si el nombre de la columna es diferente después de cargar el CSV (por ejemplo, con espacios o mayúsculas/minúsculas),\n",
    "# ajústalo aquí. Las cabeceras en el CSV son probablemente 'Day', 'Month', 'Year', 'Search Term'.\n",
    "# Spark a menudo reemplaza espacios con guiones bajos o los maneja, pero es bueno verificar.\n",
    "# Por ahora, asumiré que Spark la cargó como 'searchterm' o 'Search Term'.\n",
    "# Si la cabecera original es 'Search Term', Spark podría haberla cargado como `Search Term` o `Search_Term`\n",
    "\n",
    "# Primero, veamos los nombres exactos de las columnas después de la carga\n",
    "print(f\"Nombres de columna reales: {search_df.columns}\")\n",
    "\n",
    "# Asumiendo que la columna se llama 'searchterm' o 'Search Term'\n",
    "# Ajusta el nombre de la columna si es necesario después de ver search_df.columns\n",
    "column_name_to_check = None\n",
    "if 'searchterm' in search_df.columns:\n",
    "    column_name_to_check = 'searchterm'\n",
    "elif 'Search Term' in search_df.columns: # El CSV original probablemente tiene esta cabecera\n",
    "    column_name_to_check = 'Search Term'\n",
    "elif 'Search_Term' in search_df.columns: # Spark a veces reemplaza espacios con _\n",
    "    column_name_to_check = 'Search_Term'\n",
    "\n",
    "\n",
    "if column_name_to_check:\n",
    "    searchterm_dtype = search_df.schema[column_name_to_check].dataType\n",
    "    print(f\"El tipo de dato de la columna '{column_name_to_check}' es: {searchterm_dtype}\")\n",
    "else:\n",
    "    print(\"ERROR: La columna 'searchterm' (o una variante) no fue encontrada. Por favor, verifica los nombres de columna.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El término 'gaming laptop' fue buscado 499 veces.\n"
     ]
    }
   ],
   "source": [
    "# How many times was the term `gaming laptop` searched?\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Asegúrate de usar el nombre correcto de la columna que contiene los términos de búsqueda\n",
    "# Asumiremos que es column_name_to_check de la celda anterior, o directamente 'Search Term'\n",
    "# si lo confirmaste.\n",
    "# Si la columna se cargó como `Search Term` (con espacio), necesitas usar comillas invertidas: col(\"`Search Term`\")\n",
    "\n",
    "# Primero, verifica el nombre exacto de la columna de términos de búsqueda de la celda anterior\n",
    "# Si es 'Search Term' (con espacio):\n",
    "search_term_column_actual_name = \"Searchterm\" # Ajusta si es diferente\n",
    "\n",
    "count_gaming_laptop = search_df.filter(col(f\"`{search_term_column_actual_name}`\") == \"gaming laptop\").count()\n",
    "\n",
    "print(f\"El término 'gaming laptop' fue buscado {count_gaming_laptop} veces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 5 términos de búsqueda más frecuentes son:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===================================================>   (189 + 9) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|Searchterm   |count|\n",
      "+-------------+-----+\n",
      "|mobile 6 inch|2312 |\n",
      "|mobile 5g    |2301 |\n",
      "|mobile latest|1327 |\n",
      "|laptop       |935  |\n",
      "|tablet wifi  |896  |\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the top 5 most frequently used search terms?\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# De nuevo, usa el nombre correcto de la columna de términos de búsqueda\n",
    "# search_term_column_actual_name = \"Search Term\" # (del paso anterior)\n",
    "\n",
    "top_5_search_terms = search_df.groupBy(f\"`{search_term_column_actual_name}`\") \\\n",
    "                              .count() \\\n",
    "                              .orderBy(desc(\"count\")) \\\n",
    "                              .limit(5)\n",
    "\n",
    "print(\"Los 5 términos de búsqueda más frecuentes son:\")\n",
    "top_5_search_terms.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-07 03:50:29--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.tar.gz\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1490 (1.5K) [application/x-tar]\n",
      "Saving to: ‘model.tar.gz’\n",
      "\n",
      "model.tar.gz        100%[===================>]   1.46K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-07 03:50:29 (6.16 MB/s) - ‘model.tar.gz’ saved [1490/1490]\n",
      "\n",
      "-rw-r--r-- 1 jupyterlab resources 1490 Sep 29  2022 model.tar.gz\n",
      "total 4\n",
      "drwxr-xr-x 1 jupyterlab resources 4096 Mar 16  2022 sales_prediction.model\n"
     ]
    }
   ],
   "source": [
    "# The pretrained sales forecasting model is available at the below url\n",
    "# https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.tar.gz\n",
    "\n",
    "model_file_name = \"model.tar.gz\"\n",
    "model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.tar.gz\"\n",
    "\n",
    "# Descargar el modelo\n",
    "!wget -O {model_file_name} {model_url}\n",
    "\n",
    "# Verificar\n",
    "!ls -l {model_file_name}\n",
    "\n",
    "# Los modelos de SparkML a menudo se guardan como directorios.\n",
    "# Un archivo .tar.gz es un archivo comprimido. Necesitamos descomprimirlo y extraerlo.\n",
    "# El nombre del directorio del modelo suele ser el mismo que el del archivo sin .tar.gz, o\n",
    "# puede estar contenido dentro del tar.\n",
    "# Vamos a crear un directorio y extraerlo allí.\n",
    "model_dir_name = \"sales_forecast_model_dir\" # Nombre del directorio donde extraeremos el modelo\n",
    "!mkdir -p {model_dir_name}\n",
    "!tar -xzf {model_file_name} -C {model_dir_name} # Extraer en el directorio especificado\n",
    "\n",
    "# Verificar el contenido del directorio del modelo\n",
    "!ls -l {model_dir_name}\n",
    "# Deberías ver subdirectorios como 'metadata', 'stages', etc., si es un modelo de Pipeline.\n",
    "# O 'metadata' y 'data' si es un modelo simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido del directorio del modelo (sales_forecast_model_dir/sales_prediction.model):\n",
      "total 8\n",
      "drwxr-xr-x 1 jupyterlab resources 4096 Mar 16  2022 data\n",
      "drwxr-xr-x 1 jupyterlab resources 4096 Mar 16  2022 metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de pronóstico de ventas (LinearRegressionModel) cargado exitosamente desde: sales_forecast_model_dir/sales_prediction.model\n",
      "Coeficientes del modelo: [6.522567861288859]\n",
      "Intercepto del modelo: -13019.989140447298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the sales forecast model.\n",
    "from pyspark.ml.regression import LinearRegressionModel # <--- CAMBIO IMPORTANTE AQUÍ\n",
    "\n",
    "# model_dir_name fue definido en la celda anterior como \"sales_forecast_model_dir\"\n",
    "path_to_model_files = f\"{model_dir_name}/sales_prediction.model\"\n",
    "\n",
    "print(f\"Contenido del directorio del modelo ({path_to_model_files}):\")\n",
    "!ls -l {path_to_model_files}\n",
    "\n",
    "try:\n",
    "    sales_model = LinearRegressionModel.load(path_to_model_files) # <--- CAMBIO IMPORTANTE AQUÍ\n",
    "    print(f\"Modelo de pronóstico de ventas (LinearRegressionModel) cargado exitosamente desde: {path_to_model_files}\")\n",
    "    # Puedes imprimir algunas propiedades del modelo para verificar\n",
    "    print(f\"Coeficientes del modelo: {sales_model.coefficients}\")\n",
    "    print(f\"Intercepto del modelo: {sales_model.intercept}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el modelo: {e}\")\n",
    "    print(\"Verifica la ruta y el tipo de modelo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame para predicción:\n",
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|[2023.0]|\n",
      "+--------+\n",
      "\n",
      "Predicción de ventas para 2023:\n",
      "+------------------+\n",
      "|        prediction|\n",
      "+------------------+\n",
      "|175.16564294006457|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the sales forecast model, predict the sales for the year of 2023.\n",
    "# Para hacer una predicción, necesitamos crear un DataFrame con las características\n",
    "# que el modelo espera. No sabemos qué características son sin más información\n",
    "# sobre el modelo.\n",
    "# Asumamos que el modelo espera una característica de 'year' o alguna característica temporal.\n",
    "\n",
    "# Esto es MUY ESPECULATIVO y probablemente necesite ser ajustado\n",
    "# basado en las características reales que el modelo 'sales_model' espera.\n",
    "\n",
    "# Si el modelo espera una columna 'features' que es un vector del año:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Crear un DataFrame con el año para el que queremos predecir\n",
    "year_to_predict_data = [(2023,)] # Asumiendo que el modelo toma el año como una característica escalar\n",
    "year_df_schema = [\"year_feature\"] # Nombre de la columna temporal\n",
    "predict_df_raw = spark.createDataFrame(year_to_predict_data, year_df_schema)\n",
    "\n",
    "# Si el modelo (o su pipeline) espera una columna llamada 'features'\n",
    "# y esta se crea a partir de 'year_feature':\n",
    "try:\n",
    "    \n",
    "    \n",
    "    assembler_pred = VectorAssembler(inputCols=[\"year_feature\"], outputCol=\"features_for_pred\")\n",
    "    predict_df_assembled = assembler_pred.transform(predict_df_raw)\n",
    "    \n",
    "    # Seleccionar solo la columna de características si el modelo la espera directamente\n",
    "    # (y no es un PipelineModel que la busque desde el nombre original)\n",
    "    # predict_df_final = predict_df_assembled.select(\"features_for_pred\")\n",
    "    # Renombrar 'features_for_pred' a 'features' si el modelo espera 'features'\n",
    "    predict_df_final = predict_df_assembled.withColumnRenamed(\"features_for_pred\", \"features\").select(\"features\")\n",
    "\n",
    "    print(\"DataFrame para predicción:\")\n",
    "    predict_df_final.show()\n",
    "    \n",
    "    # Hacer la predicción\n",
    "    # La columna de predicción suele llamarse 'prediction' por defecto\n",
    "    forecast = sales_model.transform(predict_df_final) # Usar predict_df_raw si sales_model es un Pipeline que maneja el ensamblaje\n",
    "                                                        # Usar predict_df_final si sales_model es un estimador que espera \"features\"\n",
    "    \n",
    "    print(\"Predicción de ventas para 2023:\")\n",
    "    forecast.select(\"prediction\").show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al hacer la predicción: {e}\")\n",
    "    print(\"No se pudo realizar la predicción. Esto puede deberse a que:\")\n",
    "    print(\"1. El modelo no se cargó correctamente.\")\n",
    "    print(\"2. El DataFrame de entrada para la predicción no tiene el esquema/características que el modelo espera.\")\n",
    "    print(\"   - Necesitamos saber qué columnas de entrada y en qué formato (ej. vector 'features') espera el 'sales_model'.\")\n",
    "    print(\"   - Si 'sales_model' es un PipelineModel, podría esperar las columnas originales antes del ensamblaje.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
